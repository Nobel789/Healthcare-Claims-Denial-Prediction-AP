{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xtYt0UfKTEA"
      },
      "source": [
        "This  notebook script generates synthetic medical claims, introduces logical errors (like doing knee surgery for a common cold), and trains a model to catch them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZcKcoo-KOrM",
        "outputId": "24a4617a-c389-4004-c6d2-cba334f1fc6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- MODEL PERFORMANCE ---\n",
            "Accuracy: 0.965\n",
            "\n",
            "--- TOP REASONS FOR DENIAL (Feature Importance) ---\n",
            "           feature  importance\n",
            "4  diagnosis_match    0.443178\n",
            "3     has_pre_auth    0.303906\n",
            "2     service_code    0.136677\n",
            "0              age    0.100940\n",
            "1    provider_type    0.015299\n",
            "\n",
            "--- TEST CASE ---\n",
            "Bad Claim Prediction: DENIED (Confidence: 100.0%)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# --- 1. DATA GENERATION (The \"Real World\" Simulation) ---\n",
        "np.random.seed(42)\n",
        "n_claims = 1000\n",
        "\n",
        "# Generate random claim data\n",
        "data = pd.DataFrame({\n",
        "    'claim_id': range(n_claims),\n",
        "    'age': np.random.randint(18, 90, n_claims),\n",
        "    'provider_type': np.random.choice(['General', 'Surgeon', 'Cardiologist'], n_claims),\n",
        "    'service_code': np.random.choice(['Office Visit', 'Knee Surgery', 'Heart Surgery'], n_claims),\n",
        "    'has_pre_auth': np.random.choice([0, 1], n_claims, p=[0.3, 0.7]), # 30% missing pre-auth\n",
        "    'diagnosis_match': np.random.choice([0, 1], n_claims, p=[0.2, 0.8]) # 20% mismatch diagnosis\n",
        "})\n",
        "\n",
        "# --- 2. LOGIC FOR DENIAL (The \"Ground Truth\") ---\n",
        "# We define the rules the AI must \"learn\" purely from looking at the data.\n",
        "def determine_denial(row):\n",
        "    # Rule 1: Surgeries REQUIRE Pre-Auth. If missing, DENY.\n",
        "    if 'Surgery' in row['service_code'] and row['has_pre_auth'] == 0:\n",
        "        return 1\n",
        "    # Rule 2: Diagnosis must match procedure. If mismatch, DENY.\n",
        "    if row['diagnosis_match'] == 0:\n",
        "        return 1\n",
        "    # Rule 3: Random administrative errors (5% chance)\n",
        "    if np.random.random() < 0.05:\n",
        "        return 1\n",
        "    return 0 # Approved\n",
        "\n",
        "data['is_denied'] = data.apply(determine_denial, axis=1)\n",
        "\n",
        "# --- 3. PREPROCESSING (Linear Algebra Step) ---\n",
        "# Computers can't read strings like 'Surgeon'. We convert them to numbers (Encoding).\n",
        "# In real life, we use One-Hot Encoding. Here, we use simple mapping for readability.\n",
        "data_encoded = data.copy()\n",
        "data_encoded['provider_type'] = data_encoded['provider_type'].map({'General': 0, 'Surgeon': 1, 'Cardiologist': 2})\n",
        "data_encoded['service_code'] = data_encoded['service_code'].map({'Office Visit': 0, 'Knee Surgery': 1, 'Heart Surgery': 2})\n",
        "\n",
        "# Features (X) vs Target (y)\n",
        "X = data_encoded[['age', 'provider_type', 'service_code', 'has_pre_auth', 'diagnosis_match']]\n",
        "y = data_encoded['is_denied']\n",
        "\n",
        "# Split into Training (80%) and Testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 4. MODEL TRAINING (The \"Learning\" Step) ---\n",
        "# We use a Random Forest. It uses \"Information Gain\" (Entropy) to find the best rules.\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- 5. EVALUATION (The Scorecard) ---\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# --- 6. EXPLAINABILITY (Why?) ---\n",
        "# Extract which features mattered most\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# --- OUTPUT RESULTS ---\n",
        "print(\"--- MODEL PERFORMANCE ---\")\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))\n",
        "print(\"\\n--- TOP REASONS FOR DENIAL (Feature Importance) ---\")\n",
        "print(feature_importance)\n",
        "\n",
        "print(\"\\n--- TEST CASE ---\")\n",
        "# Let's test a specific BAD claim: Knee Surgery, No Pre-Auth\n",
        "bad_claim = pd.DataFrame([[65, 1, 1, 0, 1]], columns=['age', 'provider_type', 'service_code', 'has_pre_auth', 'diagnosis_match'])\n",
        "pred = model.predict(bad_claim)[0]\n",
        "prob = model.predict_proba(bad_claim)[0][1]\n",
        "print(f\"Bad Claim Prediction: {'DENIED' if pred==1 else 'APPROVED'} (Confidence: {prob:.1%})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# 1. Train your real model (you already have this)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 2. Save the model to a file\n",
        "joblib.dump(model, 'denial_model.pkl') \n",
        "\n",
        "print(\"Model saved as denial_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1pLduHMKxhe"
      },
      "source": [
        "Understanding the Output\n",
        "\n",
        "When you run this code, you will typically see results like this:\n",
        "1. Feature Importance (The \"Why\")\n",
        "\n",
        "The model will output a table showing which columns drove the decision. You will notice that service_code, has_pre_auth, and diagnosis_match have very high importance scores (e.g., 0.30 - 0.40), while age and provider_type have very low scores.\n",
        "\n",
        "    Math Insight: The algorithm calculated the \"Gini Impurity.\" It realized that splitting the data based on has_pre_auth cleared up the confusion much faster than splitting by age.\n",
        "\n",
        "2. The Test Case\n",
        "\n",
        "We fed it a Bad Claim: Knee Surgery (1), No Pre-Auth (0).\n",
        "\n",
        "    Prediction: DENIED\n",
        "\n",
        "    Confidence: ~99%\n",
        "\n",
        "    Why? The model successfully \"learned\" Rule #1 from our data generation step without us explicitly programming the if statement into the model itself.\n",
        "\n",
        "How this maps to the Math Pipeline\n",
        "\n",
        "    Vectorization: data_encoded turned \"Knee Surgery\" into [1]. The model sees a vector [65, 1, 1, 0, 1].\n",
        "\n",
        "    Calculus (Optimization): During model.fit, the Random Forest tried thousands of splits. It minimized the \"Entropy\" (chaos) in the leaf nodes.\n",
        "\n",
        "    Probability: predict_proba looked at how many of the 100 trees in the forest voted for \"Denial.\" If 99 trees said \"Deny,\" the probability is 0.99."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
